{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/header.png\" alt=\"Header\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Predictive Maintenance using NVIDIA RAPIDS and Deep Learning Models</h1>\n",
    "<h4 align=\"center\">Part 2: Training GPU LSTM models using Keras+Tensorflow for Time Series</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab, we utilized accelerated XGBoost for classification of faulty hard drives. In this lab, we deal with data points gathered from hard drives as time series and use recurrent neural networks to predict hard disk failures. \n",
    "\n",
    "Companies are looking at implementing \"Predictive Maintenance Procedures\" to avoid downtimes by predicting the failure of a device before it happens so they can repair/replace the device, preventing costly downtimes.  \n",
    "\n",
    "In this exercise, we will leverage the Backblaze Hard Drive SMART data to train an LSTM model that will predict potential future failures.  Unlike the previous exercise, where the XGBoost model predicted based on current data, the LSTM model will leverage time series data to look for trends prior to a failure.\n",
    "\n",
    "One observation that comes from this exercise is that most of the work to train this model revolves around the data preparation.  We will walk through creating the \"normal\" and \"failing\" sequences that will indicate when we expect to see early warning signs of a disk failure.  Then we will use that data to train our model.\n",
    "\n",
    "This lab covers the following topics:\n",
    "\n",
    "* [Recurrent Networks: Long Short-Term Memory](#1)\n",
    "* [Vanishing Gradient Problem and LSTMs](#2)\n",
    "* [LSTM Cell Structure](#3)\n",
    "* [LSTM Applications](#4)\n",
    "* [Preparing the Environment](#5)\n",
    "* [Load Datasets for Data Preparation](#6)\n",
    "* [Preparing the Training Data](#7)\n",
    "* [Creating X and y labels](#8)\n",
    "    * [ Exercise 1: Create class labels](#e1)\n",
    "    * [ Exercise 2: Reshaping the training data](#e2)\n",
    "* [Model Design](#9)\n",
    "* [Training the Model](#10)\n",
    "* [Confusion Matrix](#11)\n",
    "* [Optional:  CNN LSTM Architecture](#12)\n",
    "\n",
    "Let's first discuss LSTM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Recurrent Networks: Long Short-Term Memory\n",
    "\n",
    "In this section, we present a brief overview of Recurrent Neural Networks (RNNs) and the Long short-term memory (LSTM) variation of RNNs, which is used in this lab to predict hard disk failures. Note that an in-depth understanding of the internal operations of LSTMs is not required to follow the application presented. However, at the end of this section, you are provided with external links for more in-depth analysis of such models and we encourage the students to read through them in order to have a better grasp of the architecture behind RNN networks.\n",
    "\n",
    "RNNs were created to solve the memory-persistence issue found in conventional feedforward networks. Inspired by the human mind, which does not initiate the thinking process from scratch and instead uses memory-persistence based on the sequential order of events, RNNs were devised to possess such a characteristic. To accomplish this, RNNs maintain a loop that allows the information to be persistent in the network. Figure 1 depicts an RNN network where at each time step $t$, the network receives input $X_t$ and outputs a value of $y_t$. The left side of the figure shows an RNN cell with input $X$ and output $y$. There is also a loop-back of the state, which represents the memory. At a given timestep $t$, the output also depends on all previous timesteps $t-1, t-2, ..., 0$. This can also be thought of as an unrolled network with each timestep being input to a different cell and horizontal connections between cells. These connections represent the state that is passed on to the next timestep. Each timestep generates its own output that can be fed into another layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lstm seq.png\" style=\"margin-top:10px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 1. The unrolled recurrent neural network</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the conventional RNNs tend to look at information stored a long time ago, often we are interested in more recent data points, which in most cases suffice to predict the next state. Another well-studied issue with RNNs is the gradient vanishing-exploding problem. In a multi-layer network, gradients are calculated as a product of gradients of many activation functions. Consequently, when those gradients are close to zero, the gradient vanishes. On the other hand, when the previous gradients are bigger, it explodes. We will return to this topic later in this section.\n",
    "\n",
    "__LSTM__ is a type of Recurrent Neural Network especially designed to prevent the neural network output for a given input from either decaying or exploding as it cycles through the feedback loops. The feedback loops are what allow recurrent networks to be better at pattern recognition than other neural networks. \n",
    " \n",
    "Length of memory of past input is critical for solving sequence learning tasks and Long short-term memory networks provide better performance compared to other RNN architectures by alleviating what is called the vanishing gradient problem.\n",
    "\n",
    "Similar to the RNN model shown above, the LSTM Architecture consists of linear units with a self-connection that allows the flow of a value (forward pass) or gradient (backward pass) to be preserved and subsequently retrieved at the required time step. The self-recurrent unit, the memory cell, is capable of storing information which lies a dozen of time-steps in the past. This is very powerful for many tasks. For example, for text data, an LSTM unit can store information contained in the previous paragraph and apply this information to a sentence in the current paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Vanishing Gradient Problem and LSTMs\n",
    "\n",
    "A simple RNN model only has a single hidden RNN layer while a stacked RNN model (needed for advanced applications) has multiple RNN hidden layers. A common problem in deep networks is the “vanishing gradient” problem, where the gradient gets smaller and smaller with each layer until it is too small to affect the deepest layers. \n",
    "\n",
    "This is because small gradients or weights (values less than 1) are multiplied many times over through the multiple time steps, and the gradients shrink asymptotically to zero. This means the weights of those earlier layers won’t be changed significantly and, therefore, the network won’t learn long-term dependencies.\n",
    "\n",
    "The gradients of neural networks are found during backpropagation.  When we multiply lots of -1 << real numbers << 1 together, we end up with a vanishing product, which leads to a very small value and hence practically no learning of the weight values – the predictive power of the neural network then plateaus.\n",
    "\n",
    "With the memory cell in LSTMs, we have continuous gradient flow (errors maintain their value) which thus eliminates the vanishing gradient problem and enables learning from sequences which are hundreds of time steps long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM models are powerful enough to learn the most important past behaviours and understand whether or not those past behaviours are important features in making future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## LSTM Cell Structure\n",
    "\n",
    "An LSTM cell block is made of various components called the cell state, the input gate, the forget gate, and the output gate.  The cell state runs along the entire length of the cell and interacts with each of the gates.  The gates are a way to control the information that gets through to the cell state.\n",
    "\n",
    "Here is a graphical representation of the LSTM cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/LSTM cell.png\" style=\"margin-top:10px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 2. LSTM Cell</p> <p style=\"text-align: center;color:gray;font-size:12px;\"> Image from book: <i>Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems</i>\n",
    " by Aurélien Géron </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice first, on the bottom, we have our new sequence value $x_t$ being concatenated to the previous output from the cell $h_{t-1}$. The first step for this combined input is for it to be squashed via a $tanh$ layer. Next, the output of the $tanh$ operation is passed through an input gate. An input gate is a layer of sigmoid activated nodes whose output is multiplied by the squashed input. These gate sigmoids can act to “kill off” any elements of the input vector that aren’t required and need to be forgotten. A sigmoid function outputs values between 0 and 1, so the weights connecting the input to these nodes can be trained to output values close to zero to “switch off” certain input values (or, conversely, outputs close to 1 to “pass through” other values).\n",
    "\n",
    "The next step in the flow of data through this cell is the internal state / forget gate loop. LSTM cells have an internal state variable $c_t$. This variable lagged one time step, i.e. $c_{t-1}$, is added to the input data to create an effective layer of recurrence. This addition operation, instead of a multiplication operation, helps to reduce the risk of vanishing gradients. However, this recurrence loop is controlled by a forget gate. This works the same as the input gate but instead helps the network learn which state variables should be “remembered” or “forgotten”.\n",
    "\n",
    "As the final gate, we have an output layer $tanh$ squashing function (re-scale the numbers between -1 and 1), the output of which is controlled by an output gate. This gate determines which values are actually allowed as an output from the cell $h_t$.\n",
    "\n",
    "Last, running along the top of the cell is the cell state, that leverage pointwise multiplication operations to interact with each of the gates.  \n",
    "\n",
    "\n",
    "To have a more in-depth understanding of LSTM networks and relative analysis, we recommend the curious audience to read the following articles:\n",
    "\n",
    " - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    " \n",
    " - [Tutorial for Implementing Long Short-Term Memory](https://heartbeat.fritz.ai/a-beginners-guide-to-implementing-long-short-term-memory-networks-lstm-eb7a2ff09a27)\n",
    " \n",
    " - [LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## LSTM Applications\n",
    "\n",
    "LSTMs, due to their ability to learn long term dependencies, are applicable to a number of sequence learning problems including language modelling and translation, acoustic modelling of speech, speech synthesis, speech recognition, audio and video data analysis, handwriting recognition and generation, sequence prediction, and protein secondary structure prediction.\n",
    "\n",
    "In this lab, we apply the LSTM model to the Backblaze data we introduced in the previous lab to create a model that predicts the hard disk failure based on the recorded data-points at least a few days before the actual failure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## Preparing the Environment\n",
    "\n",
    "Below we are importing the required libraries and tools we use throughout this notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Suppress TF logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# prevent keras/tf from allocating all gpu memory\n",
    "gpu = 2    # set to number of GPU we want to leverage for training\n",
    "tf.device('/device:GPU:%s' % gpu)\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## Load Datasets for Data Preparation\n",
    "\n",
    "For your convenience, we have filtered and stored the `ST4000DM000` model data points in a pickle file. In the following cell, we are reloading the pickle file into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in training and test SMART data stored in previous exercise...\n",
      "(12237899, 53)\n",
      "(3196552, 53)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dir = './data/'\n",
    "\n",
    "# Files from Lab 1\n",
    "train_pkl_file = 'Lab1-2017-Full-ST4000DM000.pkl'\n",
    "test_pkl_file = 'Lab1-2016-Q4-ST4000DM000.pkl'\n",
    "\n",
    "# Load our training and test set from previous step\n",
    "print(\"Reading in training and test SMART data stored in previous exercise...\")\n",
    "df_train = pd.read_pickle(data_dir+train_pkl_file)\n",
    "df_test = pd.read_pickle(data_dir+test_pkl_file)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at the data samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>model</th>\n",
       "      <th>capacity_bytes</th>\n",
       "      <th>failure</th>\n",
       "      <th>smart_1_normalized</th>\n",
       "      <th>smart_1_raw</th>\n",
       "      <th>smart_3_normalized</th>\n",
       "      <th>smart_3_raw</th>\n",
       "      <th>smart_4_normalized</th>\n",
       "      <th>...</th>\n",
       "      <th>smart_198_normalized</th>\n",
       "      <th>smart_198_raw</th>\n",
       "      <th>smart_199_normalized</th>\n",
       "      <th>smart_199_raw</th>\n",
       "      <th>smart_240_normalized</th>\n",
       "      <th>smart_240_raw</th>\n",
       "      <th>smart_241_normalized</th>\n",
       "      <th>smart_241_raw</th>\n",
       "      <th>smart_242_normalized</th>\n",
       "      <th>smart_242_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>Z305B2QN</td>\n",
       "      <td>ST4000DM000</td>\n",
       "      <td>4000787030016</td>\n",
       "      <td>0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>124084032.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9690.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.179327e+10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.596073e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>Z302A0YH</td>\n",
       "      <td>ST4000DM000</td>\n",
       "      <td>4000787030016</td>\n",
       "      <td>0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>22057080.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>17310.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.169678e+10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.860347e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>Z305BT0W</td>\n",
       "      <td>ST4000DM000</td>\n",
       "      <td>4000787030016</td>\n",
       "      <td>0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>119535272.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8514.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.755353e+10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.631456e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>Z302A0YE</td>\n",
       "      <td>ST4000DM000</td>\n",
       "      <td>4000787030016</td>\n",
       "      <td>0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>90592144.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18051.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.150714e+10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.376942e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>Z302PGH8</td>\n",
       "      <td>ST4000DM000</td>\n",
       "      <td>4000787030016</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>240109536.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14303.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.645696e+10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.311286e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date serial_number        model  capacity_bytes  failure  \\\n",
       "4  2017-02-01      Z305B2QN  ST4000DM000   4000787030016        0   \n",
       "7  2017-02-01      Z302A0YH  ST4000DM000   4000787030016        0   \n",
       "8  2017-02-01      Z305BT0W  ST4000DM000   4000787030016        0   \n",
       "12 2017-02-01      Z302A0YE  ST4000DM000   4000787030016        0   \n",
       "13 2017-02-01      Z302PGH8  ST4000DM000   4000787030016        0   \n",
       "\n",
       "    smart_1_normalized  smart_1_raw  smart_3_normalized  smart_3_raw  \\\n",
       "4                117.0  124084032.0                91.0          0.0   \n",
       "7                109.0   22057080.0                92.0          0.0   \n",
       "8                117.0  119535272.0                93.0          0.0   \n",
       "12               115.0   90592144.0                92.0          0.0   \n",
       "13               120.0  240109536.0                92.0          0.0   \n",
       "\n",
       "    smart_4_normalized  ...  smart_198_normalized  smart_198_raw  \\\n",
       "4                100.0  ...                 100.0            0.0   \n",
       "7                100.0  ...                 100.0            0.0   \n",
       "8                100.0  ...                 100.0            0.0   \n",
       "12               100.0  ...                 100.0            0.0   \n",
       "13               100.0  ...                 100.0            0.0   \n",
       "\n",
       "    smart_199_normalized  smart_199_raw  smart_240_normalized  smart_240_raw  \\\n",
       "4                  200.0            0.0                 100.0         9690.0   \n",
       "7                  200.0            0.0                 100.0        17310.0   \n",
       "8                  200.0            0.0                 100.0         8514.0   \n",
       "12                 200.0          564.0                 100.0        18051.0   \n",
       "13                 200.0            0.0                 100.0        14303.0   \n",
       "\n",
       "    smart_241_normalized  smart_241_raw  smart_242_normalized  smart_242_raw  \n",
       "4                  100.0   3.179327e+10                 100.0   8.596073e+09  \n",
       "7                  100.0   2.169678e+10                 100.0   1.860347e+11  \n",
       "8                  100.0   2.755353e+10                 100.0   1.631456e+10  \n",
       "12                 100.0   2.150714e+10                 100.0   2.376942e+11  \n",
       "13                 100.0   1.645696e+10                 100.0   7.311286e+10  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to further process the data before the training phase. Each \"raw\" data column has a duplicated \"normalized\" version (example. smart_1_raw and smart_2_normalized) and while values of the pairs are different because of normalization, they do not possess extra information. Similar to the previous lab, we choose to eliminate the \"normalized\" columns.\n",
    "\n",
    "We also need to sort the columns based on the `serial_number` and `date` columns. Since we are dealing with time series data, the only meaningful way to process is to have them sorted, first by the related serial_number and second by the date recorded. The `prepare_dataframe` function is intended to accomplish these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df):\n",
    "    print(\"Removing unnecessary columns\")\n",
    "    \n",
    "    # find and drop all normalized columns\n",
    "    cols = [c for c in df.columns if (c.lower().find(\"normalized\")!=-1)]\n",
    "    df=df.drop(columns=cols)\n",
    "\n",
    "    # also drop the model and capacity columns as we don't need them\n",
    "    df = df.drop(columns=['model','capacity_bytes'])\n",
    "\n",
    "    # convert string dates to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # sort data by serial number and dates so ready for sequence creation\n",
    "    print(\"Sorting the data frame based on serial number and date\")\n",
    "    df = df.sort_values(by=['serial_number', 'date'], axis=0, ascending=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # ensure no NaN's in data\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the preparation step over both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train set...\n",
      "Removing unnecessary columns\n",
      "Sorting the data frame based on serial number and date\n",
      "\n",
      "Processing test set...\n",
      "Removing unnecessary columns\n",
      "Sorting the data frame based on serial number and date\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing train set...\")\n",
    "df_train = prepare_dataframe(df_train)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Processing test set...\")\n",
    "df_test = prepare_dataframe(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences from data sets\n",
    "\n",
    "In this section, we create sequences of data based on the serial numbers. To have a better understanding of this process, we need to elaborate on the architecture of the deep learning model that consumes the data. We start with a high-level architecture at this section, and later we will elaborate the model in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/lstm_simple.png\" style=\"margin-top:10px;width:600px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 3. Basic RNN model. </p>\n",
    "\n",
    "In figure 3, a simplified version of the RNN architecture is shown. Here we have `n` time steps representing each data point in the DataFrame. Also, only the last RNN (LSTM) cell is outputting the value $y$, which in our model indicates whether the data sequence ends up in a failure state or not.\n",
    "Note that the input values $x_1, x_2, …, x_n$ are multi-dimensional values and not single scalars.\n",
    "\n",
    "One important question is how to choose the value $n$. Our dataset contains lengthy time series sequences for each hard drive. Below, we choose a sequence length of _5_ and later ask the students to experiment with other values and justify their value of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we'd create sequences from all of the data that we have.  Even with the small data sets that we have, that can take more than 4 hours.  For this lab, we will simplify the process to make the time reasonable.\n",
    "\n",
    "To create 'sequences' for training our Time Series models, we will first sort the data by serial number and date (which we did above).  This will allow us to grab groups of sequential records to create each sequence. \n",
    "\n",
    "Next, we reviewed all the records to ensure that we had data reported for every day so we could create consistent sequences.  In the case of this data set, for 2017, there were 2 holes where data wasn't reported, and we used a fill-forward technique to recreate this data.\n",
    "\n",
    "To create models to predict into the future, we will use a \"lookahead_days\" parameter to see how far in the future we want the model to be trained to identify failures.  Determining how many days to lookahead is very dependent on your environment and the nature of the failures.  For the hard drives, during the creation of the lab, we tried 14 days, but we the models we created, with the minimal amount of data we use for this lab, were not very accurate.  In looking at the visualizations from Lab 1, some of the anomalous sensor values seemed to only occur 1-2 days prior to the failure, and we have selected 2 days.  In practice, this is another parameter that can be tuned.\n",
    "\n",
    "With the data all prepared, we can create sequences.\n",
    "\n",
    "Failed Records:\n",
    "\n",
    "Next, we create failed record sequences.  To do this, we will find all of the failed disk records.  Then, we will move back \"lookahead_days\" as this will be the last record in the sequence we are creating.  Then, we will take the previous $sequence length-1$ records and combine it with this last sequence record to create a sequence of records that ends up with a failure \"lookahead_days\" later.  In the case of our data set, there are so few failed disks (~1600) that we will use all of them to train our model.\n",
    "\n",
    "Normal Records:\n",
    "\n",
    "For normal records, we will first identify all disks (by serial_number) that did not have a failure.  Then, we will select a subset of the disks (to shorten the time) to create sequences for.  To create the sequence, we will walk sequentially through the records for a given disk and create sequences from that record and the next $sequence length-1$ records.  So, our first sequence (for length 5) would be Day 1-Day 5.  Then, we will go to the next day and create a sequence of Day 2-Day6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to return serial numbers of good and bad disks\n",
    "\n",
    "def get_disk_serials(df, num_disks):\n",
    "    # Get failed serial numbers\n",
    "    failed_serials = df[df['failure'] == 1]['serial_number'].unique().tolist()\n",
    "\n",
    "    # Get serial numbers for disks that didn't fail - first remove failed disks\n",
    "    df_tmp = df[~df.serial_number.isin(failed_serials)]\n",
    "    normal_serials = df_tmp.serial_number.value_counts()[:num_disks].index.tolist()\n",
    "\n",
    "    print('Normal Disk Serials:',len(normal_serials))\n",
    "    print('Failed Disk Serials:',len(failed_serials))\n",
    "\n",
    "    return normal_serials, failed_serials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine checks all the records for a given serial number to ensure we are not skipping any days.  For consistency in training our model, we need to ensure that there is a data record for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adjust_dates(df_loc): \n",
    "    df_mod = pd.DataFrame()\n",
    "    cur_serial = df_loc['serial_number'].unique().tolist()[0]\n",
    "    col_list = df_loc.columns.tolist()\n",
    "    cur_dates = df_loc['date'].values\n",
    "    \n",
    "    # determine number of days between last record and first record\n",
    "    num_date_range = int((cur_dates[-1] - cur_dates[0]).astype('timedelta64[D]')/ np.timedelta64(1, 'D'))+1\n",
    "    \n",
    "    # do we have records for each day or are there holes ?  If so, fill them.\n",
    "    if num_date_range > cur_dates.shape[0]:\n",
    "        i_low = 0\n",
    "        \n",
    "        # step through all days to ensure next date correct\n",
    "        for i in range(cur_dates.shape[0]-1): \n",
    "            \n",
    "            # calculate number of days between current data and next data - should be 1 day\n",
    "            diff_days = int((cur_dates[i+1] - cur_dates[i]).astype('timedelta64[D]')/ np.timedelta64(1, 'D'))\n",
    "            \n",
    "            # if not 1 day, fill in missing days with forward fill\n",
    "            if diff_days > 1:\n",
    "                df_mod = df_mod.append(df_loc.iloc[i_low:i+1])\n",
    "                tmp_array = np.empty((diff_days-1,len(col_list),))\n",
    "                tmp_array[:] = np.nan\n",
    "                df_add = pd.DataFrame(tmp_array,columns=col_list)\n",
    "                df_add['date'] = [ cur_dates[i] + np.timedelta64(1, 'D')*j for j in range(1,diff_days)]\n",
    "                df_mod = df_mod.append(df_add)\n",
    "                i_low = i+1\n",
    "\n",
    "        # add missing records and use forward fill to update missing sensor data\n",
    "        df_mod = df_mod.append(df_loc.iloc[i_low:])\n",
    "        df_mod = df_mod.fillna(method=\"ffill\")\n",
    "    else:\n",
    "        df_mod = df_loc \n",
    "    \n",
    "    return df_mod "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine fills in missing data to ensure we have consistent sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_date_gaps(df, normal_serials=None, failed_serials=None):\n",
    "    df_fixed = pd.DataFrame()\n",
    "\n",
    "    serials_list = normal_serials + failed_serials \n",
    "    for i, cur_serial in enumerate(serials_list): \n",
    "        df_fixed = df_fixed.append(adjust_dates(df[df['serial_number'] == cur_serial]))\n",
    "        \n",
    "    return df_fixed.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine builds failed sequences by finding the failure and then going back \"lookahead\" days and building a sequence based on the previous \"sequence_length\" days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to return failed sequences \n",
    "def create_failed_sequences(df, sequence_length, lookahead):\n",
    "\n",
    "    failed_serials = df.serial_number.unique().tolist()\n",
    "    print(\"Number of failed serials : \", len(failed_serials)) \n",
    "\n",
    "    failed_seq_list = []\n",
    "    for serial in failed_serials:\n",
    "        df_tmp = df[df['serial_number'] == serial]\n",
    "        df_tmp = df_tmp.reset_index(drop=True)\n",
    "        num_recs = df_tmp.index.size\n",
    "        \n",
    "        # if enough records, add failed sequence\n",
    "        if num_recs > (sequence_length+lookahead): \n",
    "            # find first failure\n",
    "            df_failed = df_tmp[df_tmp['failure'] == 1]\n",
    "            \n",
    "            # find end of sequence - going back \"lookahead\" days from failure\n",
    "            idx2 = df_failed.index[0] - lookahead + 1\n",
    "            \n",
    "            # find beginning of sequence\n",
    "            idx1 = idx2 - sequence_length\n",
    "            \n",
    "            if idx1 > 0: \n",
    "                failed_seq_list.append(df_tmp.iloc[idx1:idx2,:])\n",
    "    \n",
    "    print(\"Number of failed sequences :\", len(failed_seq_list)) \n",
    "    \n",
    "    return pd.concat(failed_seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to pick some serial_numbers and create all sequences from those disks up to num_normal sequences\n",
    "def create_normal_sequences(df, sequence_length, num_normal, lookahead, day_step=1):\n",
    "    normal_seq_list = []\n",
    "    num_seq = 0\n",
    "    \n",
    "    # ensure no failed sequences\n",
    "    if df[df['failure'] == 1].index.size > 0: return None \n",
    "    \n",
    "    # get list of normal serial numbers\n",
    "    normal_serials = df.serial_number.unique().tolist()\n",
    "    \n",
    "    print(\"Number of normal serials : \", len(normal_serials)) \n",
    "    \n",
    "    for serial in normal_serials:\n",
    "        df_tmp = df[df['serial_number'] == serial]\n",
    "        num_recs = df_tmp.shape[0]\n",
    "        \n",
    "        # \n",
    "        for i in range(0, num_recs-(sequence_length+lookahead)+1, day_step):\n",
    "            if (num_seq < num_normal):\n",
    "                normal_seq_list.append(df_tmp.iloc[i:i+sequence_length])\n",
    "                num_seq += 1\n",
    "    \n",
    "    print(\"Number of normal sequences :\", len(normal_seq_list))\n",
    "    \n",
    "    return pd.concat(normal_seq_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to add column \"sequence_label\" indicating whether this was a normal or failed sequence.\n",
    "# We will use this later for as label for training.\n",
    "def label_sequence(df, label):\n",
    "    df.insert(2, 'sequence_label', np.full(df.shape[0], label), True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify key parameters - you can change and re-run to test different configurations\n",
    "sequence_length = 5          # Number of days in sequence to train to detect and predict failure\n",
    "lookahead_days = 1           # Number of days in future to predict failure\n",
    "num_normal_disks = 20        # Maximum number of normal disks to look at\n",
    "max_normal_sequences = 4000  # Maximum number of normal sequences to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining good and bad disk serial numbers\n",
      "Normal Disk Serials: 20\n",
      "Failed Disk Serials: 1061\n",
      "Total Records before fixing date gaps : (189830, 27)\n",
      "Total Records after fixing date gaps  : (194895, 27)\n",
      "Creating sequences for normal disks\n",
      "Number of normal serials :  20\n",
      "Number of normal sequences : 4000\n",
      "Creating sequences for failed disks\n",
      "Number of failed serials :  1061\n",
      "Number of failed sequences : 1041\n"
     ]
    }
   ],
   "source": [
    "# get pools of serial numbers for good and bad disks\n",
    "print(\"Determining good and bad disk serial numbers\")\n",
    "normal_serials, bad_serials = get_disk_serials(df_train, num_normal_disks)\n",
    "\n",
    "df_selected = df_train[df_train.serial_number.isin(normal_serials + bad_serials)]\n",
    "print('Total Records before fixing date gaps :', df_selected.shape)\n",
    "\n",
    "df_selected = fix_date_gaps(df_selected, normal_serials, bad_serials)\n",
    "print('Total Records after fixing date gaps  :', df_selected.shape)\n",
    "    \n",
    "# get sequences\n",
    "print(\"Creating sequences for normal disks\")\n",
    "normal_sequences = create_normal_sequences(df_selected[df_selected.serial_number.isin(normal_serials)], \n",
    "                                           sequence_length, max_normal_sequences, lookahead_days)\n",
    "\n",
    "# add field to indicate normal sequence\n",
    "label_sequence(normal_sequences, 0)\n",
    "\n",
    "print(\"Creating sequences for failed disks\")\n",
    "failure_sequences = create_failed_sequences(df_selected[df_selected.serial_number.isin(bad_serials)],\n",
    "                                            sequence_length, lookahead_days)\n",
    "\n",
    "# add field to indicate failing sequence\n",
    "label_sequence(failure_sequences, 1)\n",
    "\n",
    "# combine to creating training set\n",
    "train_samples = pd.concat([normal_sequences, failure_sequences]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few normal disk sequences.  You should see 'sequence_length' records in a row that are sequential days for the same disk.  You'll see the first sequence contains day 1-day 5 (for sequence_length 5) and the second sequence contains day 2-day 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>sequence_label</th>\n",
       "      <th>failure</th>\n",
       "      <th>smart_1_raw</th>\n",
       "      <th>smart_3_raw</th>\n",
       "      <th>smart_4_raw</th>\n",
       "      <th>smart_5_raw</th>\n",
       "      <th>smart_7_raw</th>\n",
       "      <th>smart_9_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>smart_191_raw</th>\n",
       "      <th>smart_192_raw</th>\n",
       "      <th>smart_193_raw</th>\n",
       "      <th>smart_194_raw</th>\n",
       "      <th>smart_197_raw</th>\n",
       "      <th>smart_198_raw</th>\n",
       "      <th>smart_199_raw</th>\n",
       "      <th>smart_240_raw</th>\n",
       "      <th>smart_241_raw</th>\n",
       "      <th>smart_242_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70280168.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>866352864.0</td>\n",
       "      <td>29042.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28947.0</td>\n",
       "      <td>3.047652e+10</td>\n",
       "      <td>1.482313e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45851840.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>867030746.0</td>\n",
       "      <td>29066.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28971.0</td>\n",
       "      <td>3.048050e+10</td>\n",
       "      <td>1.482649e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7463496.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>867739021.0</td>\n",
       "      <td>29089.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28994.0</td>\n",
       "      <td>3.048582e+10</td>\n",
       "      <td>1.483021e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2075272.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>868471789.0</td>\n",
       "      <td>29114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29019.0</td>\n",
       "      <td>3.049139e+10</td>\n",
       "      <td>1.483400e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209190200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>869192900.0</td>\n",
       "      <td>29138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29043.0</td>\n",
       "      <td>3.049733e+10</td>\n",
       "      <td>1.483785e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45851840.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>867030746.0</td>\n",
       "      <td>29066.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28971.0</td>\n",
       "      <td>3.048050e+10</td>\n",
       "      <td>1.482649e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7463496.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>867739021.0</td>\n",
       "      <td>29089.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28994.0</td>\n",
       "      <td>3.048582e+10</td>\n",
       "      <td>1.483021e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2075272.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>868471789.0</td>\n",
       "      <td>29114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29019.0</td>\n",
       "      <td>3.049139e+10</td>\n",
       "      <td>1.483400e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209190200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>869192900.0</td>\n",
       "      <td>29138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29043.0</td>\n",
       "      <td>3.049733e+10</td>\n",
       "      <td>1.483785e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>218546040.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>869939278.0</td>\n",
       "      <td>29161.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21984.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29066.0</td>\n",
       "      <td>3.050279e+10</td>\n",
       "      <td>1.484103e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date serial_number  sequence_label  failure  smart_1_raw  smart_3_raw  \\\n",
       "0 2017-01-01      Z300KC5L               0      0.0   70280168.0          0.0   \n",
       "1 2017-01-02      Z300KC5L               0      0.0   45851840.0          0.0   \n",
       "2 2017-01-03      Z300KC5L               0      0.0    7463496.0          0.0   \n",
       "3 2017-01-04      Z300KC5L               0      0.0    2075272.0          0.0   \n",
       "4 2017-01-05      Z300KC5L               0      0.0  209190200.0          0.0   \n",
       "1 2017-01-02      Z300KC5L               0      0.0   45851840.0          0.0   \n",
       "2 2017-01-03      Z300KC5L               0      0.0    7463496.0          0.0   \n",
       "3 2017-01-04      Z300KC5L               0      0.0    2075272.0          0.0   \n",
       "4 2017-01-05      Z300KC5L               0      0.0  209190200.0          0.0   \n",
       "5 2017-01-06      Z300KC5L               0      0.0  218546040.0          0.0   \n",
       "\n",
       "   smart_4_raw  smart_5_raw  smart_7_raw  smart_9_raw  ...  smart_191_raw  \\\n",
       "0         13.0          0.0  866352864.0      29042.0  ...            0.0   \n",
       "1         13.0          0.0  867030746.0      29066.0  ...            0.0   \n",
       "2         13.0          0.0  867739021.0      29089.0  ...            0.0   \n",
       "3         13.0          0.0  868471789.0      29114.0  ...            0.0   \n",
       "4         13.0          0.0  869192900.0      29138.0  ...            0.0   \n",
       "1         13.0          0.0  867030746.0      29066.0  ...            0.0   \n",
       "2         13.0          0.0  867739021.0      29089.0  ...            0.0   \n",
       "3         13.0          0.0  868471789.0      29114.0  ...            0.0   \n",
       "4         13.0          0.0  869192900.0      29138.0  ...            0.0   \n",
       "5         13.0          0.0  869939278.0      29161.0  ...            0.0   \n",
       "\n",
       "   smart_192_raw  smart_193_raw  smart_194_raw  smart_197_raw  smart_198_raw  \\\n",
       "0            3.0        21982.0           26.0            0.0            0.0   \n",
       "1            3.0        21982.0           26.0            0.0            0.0   \n",
       "2            3.0        21982.0           26.0            0.0            0.0   \n",
       "3            3.0        21982.0           26.0            0.0            0.0   \n",
       "4            3.0        21982.0           25.0            0.0            0.0   \n",
       "1            3.0        21982.0           26.0            0.0            0.0   \n",
       "2            3.0        21982.0           26.0            0.0            0.0   \n",
       "3            3.0        21982.0           26.0            0.0            0.0   \n",
       "4            3.0        21982.0           25.0            0.0            0.0   \n",
       "5            3.0        21984.0           25.0            0.0            0.0   \n",
       "\n",
       "   smart_199_raw  smart_240_raw  smart_241_raw  smart_242_raw  \n",
       "0            0.0        28947.0   3.047652e+10   1.482313e+11  \n",
       "1            0.0        28971.0   3.048050e+10   1.482649e+11  \n",
       "2            0.0        28994.0   3.048582e+10   1.483021e+11  \n",
       "3            0.0        29019.0   3.049139e+10   1.483400e+11  \n",
       "4            0.0        29043.0   3.049733e+10   1.483785e+11  \n",
       "1            0.0        28971.0   3.048050e+10   1.482649e+11  \n",
       "2            0.0        28994.0   3.048582e+10   1.483021e+11  \n",
       "3            0.0        29019.0   3.049139e+10   1.483400e+11  \n",
       "4            0.0        29043.0   3.049733e+10   1.483785e+11  \n",
       "5            0.0        29066.0   3.050279e+10   1.484103e+11  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_sequences.head(2*sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look \"lookahead\" days past the first failed sequence to see the actual failure in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Failure Sequence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>sequence_label</th>\n",
       "      <th>failure</th>\n",
       "      <th>smart_1_raw</th>\n",
       "      <th>smart_3_raw</th>\n",
       "      <th>smart_4_raw</th>\n",
       "      <th>smart_5_raw</th>\n",
       "      <th>smart_7_raw</th>\n",
       "      <th>smart_9_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>smart_191_raw</th>\n",
       "      <th>smart_192_raw</th>\n",
       "      <th>smart_193_raw</th>\n",
       "      <th>smart_194_raw</th>\n",
       "      <th>smart_197_raw</th>\n",
       "      <th>smart_198_raw</th>\n",
       "      <th>smart_199_raw</th>\n",
       "      <th>smart_240_raw</th>\n",
       "      <th>smart_241_raw</th>\n",
       "      <th>smart_242_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2017-02-19</td>\n",
       "      <td>S30070F5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83295096.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>531725607.0</td>\n",
       "      <td>25281.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8249.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25210.0</td>\n",
       "      <td>2.316110e+10</td>\n",
       "      <td>8.881304e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2017-02-20</td>\n",
       "      <td>S30070F5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73725216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>532565600.0</td>\n",
       "      <td>25305.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8249.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25234.0</td>\n",
       "      <td>2.316667e+10</td>\n",
       "      <td>8.884915e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2017-02-21</td>\n",
       "      <td>S30070F5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41438696.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>533423914.0</td>\n",
       "      <td>25329.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8249.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25258.0</td>\n",
       "      <td>2.317391e+10</td>\n",
       "      <td>8.888442e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2017-02-22</td>\n",
       "      <td>S30070F5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25482720.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>534282220.0</td>\n",
       "      <td>25353.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8249.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25282.0</td>\n",
       "      <td>2.318148e+10</td>\n",
       "      <td>8.892227e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2017-02-23</td>\n",
       "      <td>S30070F5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>233703728.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>535136227.0</td>\n",
       "      <td>25377.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8249.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25306.0</td>\n",
       "      <td>2.318899e+10</td>\n",
       "      <td>8.895852e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date serial_number  sequence_label  failure  smart_1_raw  \\\n",
       "49 2017-02-19      S30070F5               1      0.0   83295096.0   \n",
       "50 2017-02-20      S30070F5               1      0.0   73725216.0   \n",
       "51 2017-02-21      S30070F5               1      0.0   41438696.0   \n",
       "52 2017-02-22      S30070F5               1      0.0   25482720.0   \n",
       "53 2017-02-23      S30070F5               1      0.0  233703728.0   \n",
       "\n",
       "    smart_3_raw  smart_4_raw  smart_5_raw  smart_7_raw  smart_9_raw  ...  \\\n",
       "49          0.0          8.0          0.0  531725607.0      25281.0  ...   \n",
       "50          0.0          8.0          0.0  532565600.0      25305.0  ...   \n",
       "51          0.0          8.0          0.0  533423914.0      25329.0  ...   \n",
       "52          0.0          8.0          0.0  534282220.0      25353.0  ...   \n",
       "53          0.0          8.0          0.0  535136227.0      25377.0  ...   \n",
       "\n",
       "    smart_191_raw  smart_192_raw  smart_193_raw  smart_194_raw  smart_197_raw  \\\n",
       "49            0.0            2.0         8249.0           30.0            0.0   \n",
       "50            0.0            2.0         8249.0           29.0            0.0   \n",
       "51            0.0            2.0         8249.0           29.0            0.0   \n",
       "52            0.0            2.0         8249.0           30.0            0.0   \n",
       "53            0.0            2.0         8249.0           29.0            0.0   \n",
       "\n",
       "    smart_198_raw  smart_199_raw  smart_240_raw  smart_241_raw  smart_242_raw  \n",
       "49            0.0            0.0        25210.0   2.316110e+10   8.881304e+10  \n",
       "50            0.0            0.0        25234.0   2.316667e+10   8.884915e+10  \n",
       "51            0.0            0.0        25258.0   2.317391e+10   8.888442e+10  \n",
       "52            0.0            0.0        25282.0   2.318148e+10   8.892227e+10  \n",
       "53            0.0            0.0        25306.0   2.318899e+10   8.895852e+10  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First Failure Sequence')\n",
    "failure_sequences.head(sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the record \"lookahead\" days in the future to see that we did encounter a failure on that date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We should see a failure below 1 days after the sequence above.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>failure</th>\n",
       "      <th>smart_1_raw</th>\n",
       "      <th>smart_3_raw</th>\n",
       "      <th>smart_4_raw</th>\n",
       "      <th>smart_5_raw</th>\n",
       "      <th>smart_7_raw</th>\n",
       "      <th>smart_9_raw</th>\n",
       "      <th>smart_10_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>smart_191_raw</th>\n",
       "      <th>smart_192_raw</th>\n",
       "      <th>smart_193_raw</th>\n",
       "      <th>smart_194_raw</th>\n",
       "      <th>smart_197_raw</th>\n",
       "      <th>smart_198_raw</th>\n",
       "      <th>smart_199_raw</th>\n",
       "      <th>smart_240_raw</th>\n",
       "      <th>smart_241_raw</th>\n",
       "      <th>smart_242_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16559</th>\n",
       "      <td>2017-02-24</td>\n",
       "      <td>S30070F5</td>\n",
       "      <td>1</td>\n",
       "      <td>184511376.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>535953309.0</td>\n",
       "      <td>25401.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8249.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25330.0</td>\n",
       "      <td>2.319425e+10</td>\n",
       "      <td>8.899058e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date serial_number  failure  smart_1_raw  smart_3_raw  \\\n",
       "16559 2017-02-24      S30070F5        1  184511376.0          0.0   \n",
       "\n",
       "       smart_4_raw  smart_5_raw  smart_7_raw  smart_9_raw  smart_10_raw  ...  \\\n",
       "16559          8.0          0.0  535953309.0      25401.0           0.0  ...   \n",
       "\n",
       "       smart_191_raw  smart_192_raw  smart_193_raw  smart_194_raw  \\\n",
       "16559            0.0            2.0         8249.0           29.0   \n",
       "\n",
       "       smart_197_raw  smart_198_raw  smart_199_raw  smart_240_raw  \\\n",
       "16559            0.0            0.0            0.0        25330.0   \n",
       "\n",
       "       smart_241_raw  smart_242_raw  \n",
       "16559   2.319425e+10   8.899058e+10  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_fail_serial = failure_sequences['serial_number'].iloc[0]\n",
    "df_tmp = df_train[df_train['serial_number'] == first_fail_serial]\n",
    "first_fail_index = df_tmp[df_tmp['failure'] == 1].iloc[0]\n",
    "\n",
    "print(\"We should see a failure below\",lookahead_days, \"days after the sequence above.\")\n",
    "df_tmp[df_tmp['failure'] == 1].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's output how many training records we have.  It's not a lot of data, but the process we go through is that same whether it's 1000's or 1M's of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sequences: 5041\n"
     ]
    }
   ],
   "source": [
    "# check samples\n",
    "num_seq = int(train_samples.shape[0] / sequence_length)\n",
    "print('Training Sequences:', num_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our objects are huge and in order to prevent memory leaks, we are going to store the data, clear the objects, and retrieve them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Store filesnames for later use\n",
    "lab2_train_file = data_dir + 'Lab2-LSTM_train_data_' + str(sequence_length) + '.pkl'\n",
    "lab2_test_file =  data_dir + 'Lab2-LSTM_test_data_' + str(sequence_length) + '.pkl'\n",
    "\n",
    "# Save away training data\n",
    "train_samples.to_pickle(lab2_train_file)\n",
    "del (train_samples)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we repeat the same process for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining good and bad disk serial numbers\n",
      "Normal Disk Serials: 20\n",
      "Failed Disk Serials: 234\n",
      "Total Records before fixing date gaps : (12605, 27)\n",
      "Total Records after fixing date gaps  : (12605, 27)\n",
      "Creating sequences for normal disks\n",
      "Number of normal serials :  20\n",
      "Number of normal sequences : 1740\n",
      "Creating sequences for failed disks\n",
      "Number of failed serials :  234\n",
      "Number of failed sequences : 216\n"
     ]
    }
   ],
   "source": [
    "# get pools of serial numbers for good and bad disks\n",
    "print(\"Determining good and bad disk serial numbers\")\n",
    "normal_serials, bad_serials = get_disk_serials(df_test, num_normal_disks)\n",
    "\n",
    "df_selected = df_test[df_test.serial_number.isin(normal_serials + bad_serials)]\n",
    "print('Total Records before fixing date gaps :', df_selected.shape)\n",
    "\n",
    "df_selected = fix_date_gaps(df_selected, normal_serials, bad_serials)\n",
    "print('Total Records after fixing date gaps  :', df_selected.shape)\n",
    "    \n",
    "# get sequences\n",
    "print(\"Creating sequences for normal disks\")\n",
    "normal_sequences = create_normal_sequences(df_selected[df_selected.serial_number.isin(normal_serials)], \n",
    "                                           sequence_length, max_normal_sequences, lookahead_days)\n",
    "\n",
    "# add field to indicate normal sequence\n",
    "label_sequence(normal_sequences, 0)\n",
    "\n",
    "print(\"Creating sequences for failed disks\")\n",
    "failure_sequences = create_failed_sequences(df_selected[df_selected.serial_number.isin(bad_serials)],\n",
    "                                            sequence_length, lookahead_days)\n",
    "\n",
    "# add field to indicate failure sequence\n",
    "label_sequence(failure_sequences, 1)\n",
    "\n",
    "# combine to creating training set\n",
    "test_samples = pd.concat([normal_sequences, failure_sequences]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created samples for testing let's store them away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store away training and test samples\n",
    "test_samples.to_pickle(lab2_test_file)\n",
    "del (test_samples)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n",
    "## Creating X and y labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to cleanly read the LSTM training and test data sets in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>sequence_label</th>\n",
       "      <th>failure</th>\n",
       "      <th>smart_1_raw</th>\n",
       "      <th>smart_3_raw</th>\n",
       "      <th>smart_4_raw</th>\n",
       "      <th>smart_5_raw</th>\n",
       "      <th>smart_7_raw</th>\n",
       "      <th>smart_9_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>smart_191_raw</th>\n",
       "      <th>smart_192_raw</th>\n",
       "      <th>smart_193_raw</th>\n",
       "      <th>smart_194_raw</th>\n",
       "      <th>smart_197_raw</th>\n",
       "      <th>smart_198_raw</th>\n",
       "      <th>smart_199_raw</th>\n",
       "      <th>smart_240_raw</th>\n",
       "      <th>smart_241_raw</th>\n",
       "      <th>smart_242_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70280168.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>866352864.0</td>\n",
       "      <td>29042.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28947.0</td>\n",
       "      <td>3.047652e+10</td>\n",
       "      <td>1.482313e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45851840.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>867030746.0</td>\n",
       "      <td>29066.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28971.0</td>\n",
       "      <td>3.048050e+10</td>\n",
       "      <td>1.482649e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7463496.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>867739021.0</td>\n",
       "      <td>29089.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28994.0</td>\n",
       "      <td>3.048582e+10</td>\n",
       "      <td>1.483021e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2075272.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>868471789.0</td>\n",
       "      <td>29114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29019.0</td>\n",
       "      <td>3.049139e+10</td>\n",
       "      <td>1.483400e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>Z300KC5L</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209190200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>869192900.0</td>\n",
       "      <td>29138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21982.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29043.0</td>\n",
       "      <td>3.049733e+10</td>\n",
       "      <td>1.483785e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date serial_number  sequence_label  failure  smart_1_raw  smart_3_raw  \\\n",
       "0 2017-01-01      Z300KC5L               0      0.0   70280168.0          0.0   \n",
       "1 2017-01-02      Z300KC5L               0      0.0   45851840.0          0.0   \n",
       "2 2017-01-03      Z300KC5L               0      0.0    7463496.0          0.0   \n",
       "3 2017-01-04      Z300KC5L               0      0.0    2075272.0          0.0   \n",
       "4 2017-01-05      Z300KC5L               0      0.0  209190200.0          0.0   \n",
       "\n",
       "   smart_4_raw  smart_5_raw  smart_7_raw  smart_9_raw  ...  smart_191_raw  \\\n",
       "0         13.0          0.0  866352864.0      29042.0  ...            0.0   \n",
       "1         13.0          0.0  867030746.0      29066.0  ...            0.0   \n",
       "2         13.0          0.0  867739021.0      29089.0  ...            0.0   \n",
       "3         13.0          0.0  868471789.0      29114.0  ...            0.0   \n",
       "4         13.0          0.0  869192900.0      29138.0  ...            0.0   \n",
       "\n",
       "   smart_192_raw  smart_193_raw  smart_194_raw  smart_197_raw  smart_198_raw  \\\n",
       "0            3.0        21982.0           26.0            0.0            0.0   \n",
       "1            3.0        21982.0           26.0            0.0            0.0   \n",
       "2            3.0        21982.0           26.0            0.0            0.0   \n",
       "3            3.0        21982.0           26.0            0.0            0.0   \n",
       "4            3.0        21982.0           25.0            0.0            0.0   \n",
       "\n",
       "   smart_199_raw  smart_240_raw  smart_241_raw  smart_242_raw  \n",
       "0            0.0        28947.0   3.047652e+10   1.482313e+11  \n",
       "1            0.0        28971.0   3.048050e+10   1.482649e+11  \n",
       "2            0.0        28994.0   3.048582e+10   1.483021e+11  \n",
       "3            0.0        29019.0   3.049139e+10   1.483400e+11  \n",
       "4            0.0        29043.0   3.049733e+10   1.483785e+11  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read datasets back in cleanly\n",
    "df_train = pd.read_pickle(lab2_train_file)\n",
    "df_test = pd.read_pickle(lab2_test_file)\n",
    "\n",
    "df_train.head(sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating X and y labels, there are certain columns that we need to get rid of. As an example, we can safely remove the `date` column. While the date column values helped us creating the sequences, they cannot be used for training/inference purposes (why?).\n",
    "\n",
    "Also, the `serial_number` and `failure` columns will not be used. Discuss why these two columns are not required."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Discuss your answers here:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"e1\"></a>\n",
    "### Exercise 1: Create class labels\n",
    "\n",
    "To train the model, we need to have the class labels for each sample. Complete the \"TODO\" sections below to store the class labels for `df_train` and `df_test` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# index data will hold disk serial numbers for relevant samples. We do not need serial numbers as training or testing features\n",
    "drop_columns_list = ['date','serial_number', 'failure', 'sequence_label']\n",
    "\n",
    "x_train = df_train.drop(columns=drop_columns_list)\n",
    "x_test = df_test.drop(columns=drop_columns_list)\n",
    "\n",
    "y_train = df_train['sequence_label']\n",
    "y_test = df_test['sequence_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck, click [here](#a1) for an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we removed the normalized columns and kept the raw ones. The reason behind this was that the normalized columns are generated based on much more samples from the original set and hence, do not represent the distribution accurately. In this section, we are going to normalize values in both test and train sets. We will utilize [preprocessing.StandardScaler().fit()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) method for this purpose, but first we need to convert the Pandas DataFrame to NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# X train data\n",
    "x_train = x_train.values.astype(np.float64)\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "\n",
    "# X test data\n",
    "x_test = x_test.values.astype(np.float64)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"e2\"></a>\n",
    "### Exercise 2: Reshaping the training data\n",
    "\n",
    "Next, we need to reshape the array to get 3D data (num_samples, timesteps, num_features) needed to feed LSTM. Fix to TODO sections below to reshape the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping training and test data for LSTM\n",
      "- Original Train Shape: (25205, 24)\n",
      "- Final Train Shape:    (5041, 5, 24)\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaping training and test data for LSTM\")\n",
    "print(\"- Original Train Shape:\", x_train.shape)\n",
    "\n",
    "x_train = x_train.reshape(int(x_train.shape[0]/sequence_length), sequence_length, x_train.shape[1])\n",
    "x_test = x_test.reshape(int(x_test.shape[0]/sequence_length), sequence_length, x_test.shape[1])\n",
    "\n",
    "print(\"- Final Train Shape:   \", x_train.shape)\n",
    "data_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck, click [here](#a2) for an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create the y labels (failed or not) for both test and train sets (last column of `y_train` and `y_test`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y train data\n",
    "y_train = y_train.values[0::sequence_length]\n",
    "\n",
    "# Y test data\n",
    "y_test = y_test.values[0::sequence_length]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"9\"></a>\n",
    "## Model Design\n",
    "We've finally arrived at the neural network design! Previously, we provided a simplistic overview of the network. In this section, we are going to discuss the full network design first and then build the model using Keras.\n",
    "\n",
    "In figure 3, we showed how we utilize the output of the last LSTM node in the sequence to create the relevant output. In reality, one-layer LSTM nodes are barely enough to capture the information encoded within a sequence. Alternatively, [stacked LSTMs](https://machinelearningmastery.com/stacked-long-short-term-memory-networks/) allow capturing more data complexity. In our model, we are going to use three layers of stacked LSTMs, as shown in figure 4.\n",
    "\n",
    "\n",
    "<img src=\"img/lstm_stacked.png\" style=\"margin-top:10px;width:600px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 4.  Stacked LSTM layers. </p>\n",
    "\n",
    "There is an important difference between the bottom two layers and the top one. While all cells of the bottom two layers send their outputs to their upper layer, the topmost layer has only one output which is used for classification purposes. This is achieved by setting the `return_sequences` in the LSTM definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/rapids/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/envs/rapids/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# some parameters to control model\n",
    "dp_lvl = 0.2\n",
    "regularizer_lvl = 0.002\n",
    "units = 64\n",
    "# network design\n",
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(x_train.shape[1], x_train.shape[2]),dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences =  True ))\n",
    "model.add(LSTM(units, dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences =  True ))\n",
    "model.add(LSTM(units, dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences =  False ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `units` variable holds the number of output dimensionality for each cell. At the next level, we add three fully-connected (FC) layers. The first FC layer is followed by a [Dropout layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout). The [Dropout method](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/) is a regularization technique to reduce the possibility of overfitting during training. Using this method, some randomly selected neurons are ignored during training. During the forward pass, the selected neurons do not contribute to the activation function, and their weights are also not updated within the backward phase. The last layer has a single output which indicates whether the output of the sequence is a faulty hard drive or not. Let's add those three layers to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model.add(Dense(units, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))\n",
    "\n",
    "# Dropout is a technique used to tackle Overfitting.\n",
    "model.add(Dropout (0.2))\n",
    "model.add(Dense(units, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))\n",
    "model.add(Dense(1, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, our model is complete and ready to train. Let's first take an overall look at the model we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 5, 64)             22784     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 64)             33024     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 97,217\n",
      "Trainable params: 97,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"10\"></a>\n",
    "## Training the Model\n",
    "\n",
    "\n",
    "There are a few parameters that we need to define before proceeding with the training. We set the [learning rate](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/), the number of epochs, checkpoint file locations, [decay](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1), and more importantly define a numerical optimizer, [Adam](https://arxiv.org/abs/1412.6980), to train the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using multiple GPUs..\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "epochs_num = 50\n",
    "learning_rate = 0.001\n",
    "decay_rate = 3 * learning_rate / epochs_num\n",
    "optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None,decay = decay_rate)\n",
    "\n",
    "try:\n",
    "    gpu_model = multi_gpu_model(model, gpus=gpu, cpu_merge=False)\n",
    "    print(\"Training using multiple GPUs..\")\n",
    "except:\n",
    "    gpu_model = model\n",
    "    print(\"Training using single GPU\")\n",
    "\n",
    "gpu_model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy', 'mae'])\n",
    "\n",
    "# checkpoint\n",
    "model_fn = \"best_model.h5\"\n",
    "model_filepath=data_dir+model_fn\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [es, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model defined and data loaded, let's go ahead and train the model.\n",
    "\n",
    "Remember - Our model is learning the sequences that lead up to failure a few days before the failure occurs (imposed through the way data is labeled).  This can be used as guidance on when we might perform predictive maintenance.  In our case, that would be replacing a disk driver prior to an expected failure.\n",
    "\n",
    "<span style=\"background-color:red;color:white;padding:1px,3px\"><b>Note:</b></span> At the beginning, the weights are initialized randomly. Sometimes, specific weight initialization and network settings lead to early overfitting. Since our training data is small, this may occur more often. If you see an early convergence of accuracy to 1.00, e.g.:\n",
    "\n",
    "```\n",
    "Epoch 00002: val_acc did not improve from 1.00000\n",
    "```\n",
    "\n",
    "you need to start the training over. In the above example, notice that the epoch number 2 is still very early in the training and the network has already overfitted.\n",
    "\n",
    "Now start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/rapids/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4536 samples, validate on 505 samples\n",
      "Epoch 1/50\n",
      "4536/4536 [==============================] - 17s 4ms/step - loss: 0.4311 - acc: 0.8935 - mean_absolute_error: 0.2018 - val_loss: 9.2364 - val_acc: 0.1109 - val_mean_absolute_error: 0.9005\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.11089, saving model to ./data/best_model.h5\n",
      "Epoch 2/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.4681 - acc: 0.9101 - mean_absolute_error: 0.2174 - val_loss: 1.9079 - val_acc: 0.2911 - val_mean_absolute_error: 0.6621\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.11089 to 0.29109, saving model to ./data/best_model.h5\n",
      "Epoch 3/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.3552 - acc: 0.9374 - mean_absolute_error: 0.1708 - val_loss: 1.7157 - val_acc: 0.2832 - val_mean_absolute_error: 0.6529\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.29109\n",
      "Epoch 4/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.3457 - acc: 0.9339 - mean_absolute_error: 0.1739 - val_loss: 3.2760 - val_acc: 0.3030 - val_mean_absolute_error: 0.6685\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.29109 to 0.30297, saving model to ./data/best_model.h5\n",
      "Epoch 5/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.3454 - acc: 0.9356 - mean_absolute_error: 0.1847 - val_loss: 3.7008 - val_acc: 0.2891 - val_mean_absolute_error: 0.6811\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.30297\n",
      "Epoch 6/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.3274 - acc: 0.9458 - mean_absolute_error: 0.1648 - val_loss: 4.8284 - val_acc: 0.3782 - val_mean_absolute_error: 0.6403\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.30297 to 0.37822, saving model to ./data/best_model.h5\n",
      "Epoch 7/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2824 - acc: 0.9451 - mean_absolute_error: 0.1631 - val_loss: 1.0116 - val_acc: 0.3228 - val_mean_absolute_error: 0.5793\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.37822\n",
      "Epoch 8/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.3358 - acc: 0.9398 - mean_absolute_error: 0.1908 - val_loss: 2.1213 - val_acc: 0.3861 - val_mean_absolute_error: 0.6000\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.37822 to 0.38614, saving model to ./data/best_model.h5\n",
      "Epoch 9/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.3366 - acc: 0.9409 - mean_absolute_error: 0.2114 - val_loss: 2.0013 - val_acc: 0.3267 - val_mean_absolute_error: 0.6476\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.38614\n",
      "Epoch 10/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2554 - acc: 0.9535 - mean_absolute_error: 0.1498 - val_loss: 4.7603 - val_acc: 0.4455 - val_mean_absolute_error: 0.5924\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.38614 to 0.44554, saving model to ./data/best_model.h5\n",
      "Epoch 11/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2817 - acc: 0.9544 - mean_absolute_error: 0.1536 - val_loss: 3.6657 - val_acc: 0.4653 - val_mean_absolute_error: 0.5619\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.44554 to 0.46535, saving model to ./data/best_model.h5\n",
      "Epoch 12/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2647 - acc: 0.9592 - mean_absolute_error: 0.1435 - val_loss: 4.3866 - val_acc: 0.5287 - val_mean_absolute_error: 0.5238\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.46535 to 0.52871, saving model to ./data/best_model.h5\n",
      "Epoch 13/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2697 - acc: 0.9597 - mean_absolute_error: 0.1539 - val_loss: 3.8794 - val_acc: 0.5228 - val_mean_absolute_error: 0.5302\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.52871\n",
      "Epoch 14/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2491 - acc: 0.9612 - mean_absolute_error: 0.1400 - val_loss: 6.4194 - val_acc: 0.4693 - val_mean_absolute_error: 0.6217\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.52871\n",
      "Epoch 15/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.3097 - acc: 0.9489 - mean_absolute_error: 0.1659 - val_loss: 6.3225 - val_acc: 0.3941 - val_mean_absolute_error: 0.6913\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.52871\n",
      "Epoch 16/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.3084 - acc: 0.9632 - mean_absolute_error: 0.1564 - val_loss: 4.8109 - val_acc: 0.5426 - val_mean_absolute_error: 0.5247\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.52871 to 0.54257, saving model to ./data/best_model.h5\n",
      "Epoch 17/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2631 - acc: 0.9603 - mean_absolute_error: 0.1507 - val_loss: 2.9062 - val_acc: 0.5366 - val_mean_absolute_error: 0.4912\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.54257\n",
      "Epoch 18/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2306 - acc: 0.9630 - mean_absolute_error: 0.1378 - val_loss: 4.2308 - val_acc: 0.4772 - val_mean_absolute_error: 0.5657\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.54257\n",
      "Epoch 19/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2007 - acc: 0.9621 - mean_absolute_error: 0.1308 - val_loss: 4.0764 - val_acc: 0.5485 - val_mean_absolute_error: 0.5248\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.54257 to 0.54851, saving model to ./data/best_model.h5\n",
      "Epoch 20/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2658 - acc: 0.9678 - mean_absolute_error: 0.1349 - val_loss: 4.7619 - val_acc: 0.5782 - val_mean_absolute_error: 0.4901\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.54851 to 0.57822, saving model to ./data/best_model.h5\n",
      "Epoch 21/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2512 - acc: 0.9674 - mean_absolute_error: 0.1289 - val_loss: 4.9371 - val_acc: 0.5604 - val_mean_absolute_error: 0.5039\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.57822\n",
      "Epoch 22/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2656 - acc: 0.9473 - mean_absolute_error: 0.1466 - val_loss: 0.5677 - val_acc: 0.7109 - val_mean_absolute_error: 0.3270\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.57822 to 0.71089, saving model to ./data/best_model.h5\n",
      "Epoch 23/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2901 - acc: 0.9458 - mean_absolute_error: 0.1607 - val_loss: 1.3895 - val_acc: 0.5624 - val_mean_absolute_error: 0.4558\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.71089\n",
      "Epoch 24/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2429 - acc: 0.9464 - mean_absolute_error: 0.1732 - val_loss: 1.1699 - val_acc: 0.4772 - val_mean_absolute_error: 0.5302\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.71089\n",
      "Epoch 25/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2713 - acc: 0.9541 - mean_absolute_error: 0.1536 - val_loss: 0.9744 - val_acc: 0.4713 - val_mean_absolute_error: 0.5112\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.71089\n",
      "Epoch 26/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2340 - acc: 0.9612 - mean_absolute_error: 0.1444 - val_loss: 1.3838 - val_acc: 0.4891 - val_mean_absolute_error: 0.5138\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.71089\n",
      "Epoch 27/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.1942 - acc: 0.9630 - mean_absolute_error: 0.1254 - val_loss: 1.3021 - val_acc: 0.5505 - val_mean_absolute_error: 0.4627\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.71089\n",
      "Epoch 28/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2138 - acc: 0.9676 - mean_absolute_error: 0.1273 - val_loss: 3.6156 - val_acc: 0.5248 - val_mean_absolute_error: 0.5239\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.71089\n",
      "Epoch 29/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2210 - acc: 0.9647 - mean_absolute_error: 0.1334 - val_loss: 3.2609 - val_acc: 0.5307 - val_mean_absolute_error: 0.5195\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.71089\n",
      "Epoch 30/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2229 - acc: 0.9537 - mean_absolute_error: 0.1502 - val_loss: 2.8143 - val_acc: 0.5010 - val_mean_absolute_error: 0.5570\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.71089\n",
      "Epoch 31/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.1928 - acc: 0.9641 - mean_absolute_error: 0.1141 - val_loss: 1.8666 - val_acc: 0.5446 - val_mean_absolute_error: 0.4930\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.71089\n",
      "Epoch 32/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.1962 - acc: 0.9680 - mean_absolute_error: 0.1159 - val_loss: 4.9969 - val_acc: 0.5267 - val_mean_absolute_error: 0.5384\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.71089\n",
      "Epoch 33/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2094 - acc: 0.9652 - mean_absolute_error: 0.1209 - val_loss: 4.2565 - val_acc: 0.5485 - val_mean_absolute_error: 0.5074\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.71089\n",
      "Epoch 34/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2015 - acc: 0.9647 - mean_absolute_error: 0.1171 - val_loss: 3.0526 - val_acc: 0.5624 - val_mean_absolute_error: 0.4915\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.71089\n",
      "Epoch 35/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2351 - acc: 0.9504 - mean_absolute_error: 0.1415 - val_loss: 4.1872 - val_acc: 0.3703 - val_mean_absolute_error: 0.6891\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.71089\n",
      "Epoch 36/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2047 - acc: 0.9623 - mean_absolute_error: 0.1239 - val_loss: 3.9424 - val_acc: 0.5406 - val_mean_absolute_error: 0.5120\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.71089\n",
      "Epoch 37/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2036 - acc: 0.9643 - mean_absolute_error: 0.1207 - val_loss: 3.3408 - val_acc: 0.5426 - val_mean_absolute_error: 0.5155\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.71089\n",
      "Epoch 38/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2387 - acc: 0.9658 - mean_absolute_error: 0.1557 - val_loss: 1.5607 - val_acc: 0.4871 - val_mean_absolute_error: 0.5632\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.71089\n",
      "Epoch 39/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2145 - acc: 0.9691 - mean_absolute_error: 0.1285 - val_loss: 4.9951 - val_acc: 0.5505 - val_mean_absolute_error: 0.5293\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.71089\n",
      "Epoch 40/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2196 - acc: 0.9669 - mean_absolute_error: 0.1347 - val_loss: 5.1357 - val_acc: 0.5386 - val_mean_absolute_error: 0.5320\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.71089\n",
      "Epoch 41/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.2018 - acc: 0.9691 - mean_absolute_error: 0.1308 - val_loss: 4.8652 - val_acc: 0.5703 - val_mean_absolute_error: 0.4991\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.71089\n",
      "Epoch 42/50\n",
      "4536/4536 [==============================] - 11s 2ms/step - loss: 0.1885 - acc: 0.9733 - mean_absolute_error: 0.1262 - val_loss: 4.4823 - val_acc: 0.5941 - val_mean_absolute_error: 0.4810\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.71089\n",
      "Epoch 00042: early stopping\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "with tf.device('/device:GPU:%s' %gpu):\n",
    "    history = gpu_model.fit(x_train, y_train, epochs=epochs_num, batch_size=16, validation_split=0.1, verbose=1, shuffle=True, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the accuracy level during the training epochs. One point to observe is that at the beginning, the accuracy rate jumps pretty quickly and afterwards fluctuates heavily. This behaviour is due to our small and non-diverse training dataset. With a larger dataset which is also more uniformly distributed, you will be able to experience a smoother convergence path and higher accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show plot accuracy changes during training\n",
    "plt.plot(history.history['acc'],'g')\n",
    "plt.plot(history.history['val_acc'],'r')\n",
    "plt.title('accuracy across epochs')\n",
    "plt.ylabel('accuracy level')\n",
    "plt.xlabel('# epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to evaluate the trained model against our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1956/1956 [==============================] - 1s 677us/step\n",
      "acc: 58.90% mean_absolute_error: 49.67%\n"
     ]
    }
   ],
   "source": [
    "# load weights from best model\n",
    "gpu_model.load_weights(model_filepath)\n",
    "\n",
    "# evaluate model against test set\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy', 'mae']) \n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"%s: %.2f%% %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100, model.metrics_names[2], scores[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"11\"></a>\n",
    "## Confusion Matrix - Reminder From the Previous Lab:\n",
    "\n",
    "---\n",
    "In this section, we are going to plot the confusion matrix. While accuracy rate provides us with an overall outlook on how well our model is performing, we need more insights into the accuracy. This is especially true since our dataset is small.  We need to compromise some aspects of accuracy rate in favour of others. The `classification_report` provides us with the accuracy stats broken down into `precision`, `recall`, `f1-score`, and `support`. Before introducing these measures, let's define some abbreviations:\n",
    "\n",
    "For brevity, we are using the following abbreviations: \n",
    "- __FP__: False Positive\n",
    "- __TP__: True  Positive\n",
    "- __FN__: False Negative\n",
    "- __TN__: True  Negative\n",
    "\n",
    "The first measure is focused on identifying positive cases and is called __recall__. We define recall as the ability of the model to identify all true positive samples of the dataset. In mathematical terms, recall is the ratio of true positives over true positives plus false negatives. By other means, recall tells us, among all the test samples belonging to the output class, how many of them are identified correctly by the model:\n",
    "\n",
    "\\begin{equation*}\n",
    "recall = \\frac{TP}{TP+FN}\n",
    "\\end{equation*}\n",
    "\n",
    "The next measure, is called __precision__ and is the ability of the model to identify the relevant samples only, and is defined as the ratio of true positives over true positives plus false positives:\n",
    "\n",
    "\\begin{equation*}\n",
    "precision = \\frac{TP}{TP+FP}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Selecting a proper threshold usually stems from a good balance between the precision and recall values. A well-known measure that provides such a balance is `F1 score`, which is a harmonic mean of precision and recall, and defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "{F_1 \\: score} = 2*\\frac{precision*recall}{precision+recall}\n",
    "\\end{equation*}\n",
    "\n",
    "---\n",
    "\n",
    "One very important measure is the recall rate on defective samples. We are interested in models that retrieve as many defective hard drives as possible. The downside is that the high recall rate usually results in a lower precision rate when the dataset is small. Here we are willing to accept this pitfall in favour of a higher recall rate, given the limitation on the dataset size. We try to predict at least half of the defective hard drives. Let's measure how our model is performing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.57      0.71      1740\n",
      "           1       0.18      0.74      0.28       216\n",
      "\n",
      "    accuracy                           0.59      1956\n",
      "   macro avg       0.56      0.66      0.50      1956\n",
      "weighted avg       0.86      0.59      0.66      1956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "conf_test = y_test.reshape(y_test.shape[0],1)\n",
    "print(classification_report(conf_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.79      0.85      4000\n",
      "           1       0.49      0.78      0.60      1041\n",
      "\n",
      "    accuracy                           0.79      5041\n",
      "   macro avg       0.71      0.79      0.73      5041\n",
      "weighted avg       0.84      0.79      0.80      5041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(x_train)\n",
    "conf_train = y_train.reshape(y_train.shape[0],1)\n",
    "print(classification_report(conf_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"12\"></a>\n",
    "## Optional:  CNN LSTM Architecture\n",
    "\n",
    "In this section, we are going to introduce a more advanced architecture, officially known as a __CNN LSTM Sequence Prediction__ model. This model is designed specifically for sequential prediction with spatial inputs (e.g. videos), however, could be applied to time series prediction as well.\n",
    "\n",
    "The CNN LSTM uses a convolutional neural layer to extract features from the sequence on top of the LSTM layers which also inherently extract features. Since we are dealing with one dimensional time series, we are going to use 1D convolutional network, starting with a kernel size of 12 and 50 filters. We also use a [max pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) layer of size 2 to shrink the data size. Figure 5 shows the architecture of the new model.\n",
    "\n",
    "\n",
    "<img src=\"img/lstm_conv.png\" style=\"margin-top:10px;width:800px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 5.  CNN LSTM layers. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using multiple GPUs..\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 5, 64)             4672      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5, 64)             12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 2, 64)             33024     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 87,297\n",
      "Trainable params: 87,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# model parameters\n",
    "dp_lvl = 0.2\n",
    "regularizer_lvl = 0.002\n",
    "data_shape = x_train.shape[1:]\n",
    "\n",
    "kernel_s = 3 # should be smaller than sequence length\n",
    "filter_s = 64\n",
    "\n",
    "# network design\n",
    "model = Sequential()\n",
    "model.add(Conv1D(kernel_size = kernel_s, filters = filter_s, padding='same', input_shape=data_shape, activation='relu'))\n",
    "model.add(Conv1D(kernel_size = kernel_s, filters = filter_s, padding='same' ,activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(filter_s, dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences = True ))\n",
    "model.add(LSTM(filter_s, dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences = False ))\n",
    "model.add(Dense(filter_s, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))\n",
    "model.add(Dropout (0.2))\n",
    "model.add(Dense(1, activation='relu',activity_regularizer=regularizers.l2(regularizer_lvl)))\n",
    "\n",
    "# training parameters\n",
    "epochs_num = 100\n",
    "learning_rate = 0.001\n",
    "decay_rate = 3 * learning_rate / epochs_num\n",
    "optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None,decay = decay_rate)\n",
    "\n",
    "try:\n",
    "    gpu_model = multi_gpu_model(model, gpus=gpu, cpu_merge=False)\n",
    "    print(\"Training using multiple GPUs..\")\n",
    "except:\n",
    "    gpu_model = model\n",
    "    print(\"Training using single GPU or CPU..\")\n",
    "\n",
    "# print out model\n",
    "model.summary()\n",
    "\n",
    "# checkpoint\n",
    "model_fn = \"best_1D_model.h5\"\n",
    "model_filepath=data_dir+model_fn\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "es = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [es, checkpoint]\n",
    "\n",
    "gpu_model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy', 'mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model built, let’s train the new model to determine its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4536 samples, validate on 505 samples\n",
      "Epoch 1/100\n",
      "4536/4536 [==============================] - 13s 3ms/step - loss: 0.3651 - acc: 0.9153 - mean_absolute_error: 0.1361 - val_loss: 6.4031 - val_acc: 0.2020 - val_mean_absolute_error: 0.7750\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.20198, saving model to ./data/best_1D_model.h5\n",
      "Epoch 2/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2670 - acc: 0.9354 - mean_absolute_error: 0.1243 - val_loss: 1.0223 - val_acc: 0.3010 - val_mean_absolute_error: 0.5928\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.20198 to 0.30099, saving model to ./data/best_1D_model.h5\n",
      "Epoch 3/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1962 - acc: 0.9405 - mean_absolute_error: 0.1264 - val_loss: 1.7518 - val_acc: 0.1861 - val_mean_absolute_error: 0.6956\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.30099\n",
      "Epoch 4/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 1.0720 - acc: 0.8677 - mean_absolute_error: 0.2200 - val_loss: 9.4616 - val_acc: 0.0297 - val_mean_absolute_error: 0.9055\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.30099\n",
      "Epoch 5/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.3034 - acc: 0.9231 - mean_absolute_error: 0.1532 - val_loss: 2.2660 - val_acc: 0.2475 - val_mean_absolute_error: 0.6770\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.30099\n",
      "Epoch 6/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.7451 - acc: 0.9052 - mean_absolute_error: 0.1463 - val_loss: 10.9839 - val_acc: 0.1723 - val_mean_absolute_error: 0.8297\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.30099\n",
      "Epoch 7/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2958 - acc: 0.9506 - mean_absolute_error: 0.0877 - val_loss: 5.8646 - val_acc: 0.4515 - val_mean_absolute_error: 0.5798\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.30099 to 0.45149, saving model to ./data/best_1D_model.h5\n",
      "Epoch 8/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2444 - acc: 0.9605 - mean_absolute_error: 0.0769 - val_loss: 4.1197 - val_acc: 0.5446 - val_mean_absolute_error: 0.5157\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.45149 to 0.54455, saving model to ./data/best_1D_model.h5\n",
      "Epoch 9/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.3587 - acc: 0.9156 - mean_absolute_error: 0.1092 - val_loss: 4.4878 - val_acc: 0.4713 - val_mean_absolute_error: 0.5923\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.54455\n",
      "Epoch 10/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2436 - acc: 0.9601 - mean_absolute_error: 0.0718 - val_loss: 4.1572 - val_acc: 0.5683 - val_mean_absolute_error: 0.4770\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.54455 to 0.56832, saving model to ./data/best_1D_model.h5\n",
      "Epoch 11/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2046 - acc: 0.9687 - mean_absolute_error: 0.0559 - val_loss: 4.6285 - val_acc: 0.5861 - val_mean_absolute_error: 0.5120\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.56832 to 0.58614, saving model to ./data/best_1D_model.h5\n",
      "Epoch 12/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1735 - acc: 0.9689 - mean_absolute_error: 0.0578 - val_loss: 3.3835 - val_acc: 0.6554 - val_mean_absolute_error: 0.4716\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.58614 to 0.65545, saving model to ./data/best_1D_model.h5\n",
      "Epoch 13/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1569 - acc: 0.9698 - mean_absolute_error: 0.0665 - val_loss: 5.0890 - val_acc: 0.5446 - val_mean_absolute_error: 0.5042\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.65545\n",
      "Epoch 14/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.3028 - acc: 0.9261 - mean_absolute_error: 0.1211 - val_loss: 4.0419 - val_acc: 0.2733 - val_mean_absolute_error: 0.6859\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.65545\n",
      "Epoch 15/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2110 - acc: 0.9590 - mean_absolute_error: 0.0715 - val_loss: 3.2434 - val_acc: 0.6713 - val_mean_absolute_error: 0.3911\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.65545 to 0.67129, saving model to ./data/best_1D_model.h5\n",
      "Epoch 16/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2316 - acc: 0.9535 - mean_absolute_error: 0.0808 - val_loss: 2.2645 - val_acc: 0.6515 - val_mean_absolute_error: 0.4295\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.67129\n",
      "Epoch 17/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1995 - acc: 0.9614 - mean_absolute_error: 0.0740 - val_loss: 3.9499 - val_acc: 0.6317 - val_mean_absolute_error: 0.4397\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.67129\n",
      "Epoch 18/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2036 - acc: 0.9528 - mean_absolute_error: 0.0855 - val_loss: 5.1145 - val_acc: 0.5030 - val_mean_absolute_error: 0.5419\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.67129\n",
      "Epoch 19/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1512 - acc: 0.9665 - mean_absolute_error: 0.0833 - val_loss: 4.0836 - val_acc: 0.6634 - val_mean_absolute_error: 0.4818\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.67129\n",
      "Epoch 20/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1254 - acc: 0.9649 - mean_absolute_error: 0.0636 - val_loss: 0.4807 - val_acc: 0.8792 - val_mean_absolute_error: 0.2253\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.67129 to 0.87921, saving model to ./data/best_1D_model.h5\n",
      "Epoch 21/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2051 - acc: 0.9376 - mean_absolute_error: 0.1183 - val_loss: 3.8459 - val_acc: 0.4990 - val_mean_absolute_error: 0.5277\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87921\n",
      "Epoch 22/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1141 - acc: 0.9755 - mean_absolute_error: 0.0526 - val_loss: 3.3201 - val_acc: 0.6911 - val_mean_absolute_error: 0.4836\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87921\n",
      "Epoch 23/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.0910 - acc: 0.9738 - mean_absolute_error: 0.0493 - val_loss: 3.6172 - val_acc: 0.6792 - val_mean_absolute_error: 0.4777\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87921\n",
      "Epoch 24/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.0874 - acc: 0.9808 - mean_absolute_error: 0.0482 - val_loss: 3.7158 - val_acc: 0.6653 - val_mean_absolute_error: 0.4400\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87921\n",
      "Epoch 25/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.0884 - acc: 0.9813 - mean_absolute_error: 0.0429 - val_loss: 3.0828 - val_acc: 0.7149 - val_mean_absolute_error: 0.3680\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87921\n",
      "Epoch 26/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.0896 - acc: 0.9841 - mean_absolute_error: 0.0372 - val_loss: 3.4036 - val_acc: 0.7069 - val_mean_absolute_error: 0.4402\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87921\n",
      "Epoch 27/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.0684 - acc: 0.9863 - mean_absolute_error: 0.0363 - val_loss: 3.2931 - val_acc: 0.7188 - val_mean_absolute_error: 0.4453\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87921\n",
      "Epoch 28/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.0692 - acc: 0.9879 - mean_absolute_error: 0.0344 - val_loss: 3.1961 - val_acc: 0.7347 - val_mean_absolute_error: 0.4129\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87921\n",
      "Epoch 29/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2316 - acc: 0.9255 - mean_absolute_error: 0.1258 - val_loss: 4.4166 - val_acc: 0.0000e+00 - val_mean_absolute_error: 0.8026\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87921\n",
      "Epoch 30/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2315 - acc: 0.8999 - mean_absolute_error: 0.1419 - val_loss: 3.6747 - val_acc: 0.6376 - val_mean_absolute_error: 0.4668\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87921\n",
      "Epoch 31/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1418 - acc: 0.9645 - mean_absolute_error: 0.0624 - val_loss: 3.0710 - val_acc: 0.4772 - val_mean_absolute_error: 0.5566\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87921\n",
      "Epoch 32/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1256 - acc: 0.9694 - mean_absolute_error: 0.0530 - val_loss: 3.4797 - val_acc: 0.6000 - val_mean_absolute_error: 0.5210\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87921\n",
      "Epoch 33/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1025 - acc: 0.9751 - mean_absolute_error: 0.0445 - val_loss: 3.3656 - val_acc: 0.5129 - val_mean_absolute_error: 0.5556\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87921\n",
      "Epoch 34/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2448 - acc: 0.9528 - mean_absolute_error: 0.0810 - val_loss: 1.6626 - val_acc: 0.0000e+00 - val_mean_absolute_error: 0.7992\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87921\n",
      "Epoch 35/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.2092 - acc: 0.9550 - mean_absolute_error: 0.0716 - val_loss: 2.4213 - val_acc: 0.5941 - val_mean_absolute_error: 0.4567\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87921\n",
      "Epoch 36/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1640 - acc: 0.9813 - mean_absolute_error: 0.0435 - val_loss: 4.0568 - val_acc: 0.5941 - val_mean_absolute_error: 0.5155\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87921\n",
      "Epoch 37/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.4538 - acc: 0.9361 - mean_absolute_error: 0.1006 - val_loss: 7.0360 - val_acc: 0.4099 - val_mean_absolute_error: 0.5971\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87921\n",
      "Epoch 38/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1904 - acc: 0.9643 - mean_absolute_error: 0.0846 - val_loss: 3.8768 - val_acc: 0.4079 - val_mean_absolute_error: 0.5798\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87921\n",
      "Epoch 39/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1501 - acc: 0.9696 - mean_absolute_error: 0.0586 - val_loss: 5.7485 - val_acc: 0.4317 - val_mean_absolute_error: 0.6065\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87921\n",
      "Epoch 40/100\n",
      "4536/4536 [==============================] - 6s 1ms/step - loss: 0.1210 - acc: 0.9769 - mean_absolute_error: 0.0497 - val_loss: 3.8078 - val_acc: 0.5505 - val_mean_absolute_error: 0.5383\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87921\n",
      "Epoch 00040: early stopping\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "with tf.device('/device:GPU:%s' %gpu):\n",
    "    history = gpu_model.fit(x_train, y_train, epochs=epochs_num, batch_size=16, validation_split=0.1, verbose=1, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out by reviewing the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show plot accuracy changes during training\n",
    "plt.plot(history.history['acc'],'g')\n",
    "plt.plot(history.history['val_acc'],'r')\n",
    "plt.title('accuracy across epochs')\n",
    "plt.ylabel('accuracy level')\n",
    "plt.xlabel('# epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check out the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1956/1956 [==============================] - 1s 644us/step\n",
      "acc: 27.56% mean_absolute_error: 72.64%\n"
     ]
    }
   ],
   "source": [
    "# load weights from best model\n",
    "gpu_model.load_weights(model_filepath)\n",
    "\n",
    "# evaluate model against test set\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy', 'mae']) \n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"%s: %.2f%% %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100, model.metrics_names[2], scores[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous model, we are going to build the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.20      0.32      1740\n",
      "           1       0.12      0.92      0.22       216\n",
      "\n",
      "    accuracy                           0.28      1956\n",
      "   macro avg       0.54      0.56      0.27      1956\n",
      "weighted avg       0.86      0.28      0.31      1956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix on test set\n",
    "y_pred = model.predict_classes(x_test)\n",
    "conf_test = y_test.reshape(y_test.shape[0],1)\n",
    "print(classification_report(conf_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging the model for calculating \"Remaining Useful Life\" (RUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case above, we developed a \"classifier\" which will indicate, based on SMART drive data, that a given hard drive will fail within the \"sequnce_length\".  By increasing the sequence_length, we could potentially learn to detect failures further in the future. In addition, different failure types could have different failure windows, so an ensemble of models, each with different windows and sequences could be leveraged for more advanced RUL predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3034"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free up memory for upcoming exercises\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What do you think about the model accuracy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Discuss some ways that you could improve the performance of the model above ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"assess\"></a>\n",
    "## Assessment – Predicting Time Series\n",
    "\n",
    "The RNN model we created in this lab was designed to output a single value, indicating whether the subject time series results in a \"failed\" hard drive or not. To accomplish this, a simple class label is returned by the model through the below line of Keras code:\n",
    "\n",
    "```\n",
    "model.add(Dense(1, activation='relu',activity_regularizer=regularizers.l2(regularizer_lvl)))\n",
    "```\n",
    "\n",
    "In this assignment, we intend to perform a slightly different approach. For example, while previously we were given with sequences of length \"5\" (the original value), now we assume that the length of the sequence is one less, e.g. 4 (these are arbitrary lengths for the sake of example) in our case. Predicting the 5th element of the sequence is the subject of our model. By other means, we want the model to return a single (multi-dimensional) vector, (a vector of size [1 x 47] for the above DataFrame. You should determine the vector size by the array shape). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img/Lab2-Assessment-Example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the above criteria, there are some other changes required for the architecture of the model:\n",
    "\n",
    "1- The model starts with two stacked LSTM layers with a unit size of 128.\n",
    "\n",
    "2- The last output of the second LSTM layer is fed into a Dense model with the following properties: \n",
    "    - The number of outputs is 256\n",
    "    - The activation function is `tanh`\n",
    "    \n",
    "3- The fourth layer is a dropout layer with dropout probability of 0.2\n",
    "\n",
    "4- Next is another dense model with:\n",
    "    - The number of outputs is 128\n",
    "    - The activation function is `tanh`\n",
    "    \n",
    "5- The last layer is a Dense layer where the number of outputs is equal to the dimension of a single row of data. (Remember that we are trying to predict the values of a multi dimension vector).\n",
    "    \n",
    "The function should simply return the above model.\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__To do so, open [this python file](/lab/edit/assignment.py)__  and complete the `get_model()` function by filling in the __<<< TODO >>>__ section according to the criteria described above.\n",
    "\n",
    "\n",
    "Once done, you can use the following block of code to verify that your implementation is correct. Note that this is __NOT__ the last step. After you succeed the test below, you need to click on assess test button on the course page. For more details, read the red note below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": 0, \"message\": \"Error running assessment: list index out of range\"}\n"
     ]
    }
   ],
   "source": [
    "!python try_assessment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs provide a powerful way to analyze time series data.  While XGBoost was able to predict failure for a given set of SMART data, the LSTM gave us the ability to predict future disk failures before they happen.\n",
    "\n",
    "One thing to remember: The amount of data, model complexity, the number of features, and the number of epochs have been reduced in this tutorial to reduce the computational (and time) burden.  If we were leveraging this model for production usage, we would train on a much larger data set.  \n",
    "\n",
    "This training would take hours to days, and we would regularly retrain the model leveraging additional data as it became available.  Also, a key thing to remember is that the data is device and environment-dependent, so any major changes would require retraining to ensure the model was still appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"All-answers\"></a>\n",
    "## Answers to selected exercises:\n",
    "---\n",
    "\n",
    "<a name=\"a1\"></a>\n",
    "**Exercise 1: Create class labels:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train = df_train.drop(columns=drop_columns_list)\n",
    "x_test = df_test.drop(columns=drop_columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](#e1) to go back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"a2\"></a>\n",
    "**Exercise 2: Reshaping the training data**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train = x_train.reshape(int(x_train.shape[0]/sequence_length), sequence_length, x_train.shape[1])\n",
    "x_test = x_test.reshape(int(x_test.shape[0]/sequence_length), sequence_length, x_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](#e2) to go back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
